{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install -q transformers datasets sentence-transformers faiss-gpu chromadb tqdm openai gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:21:33.969218Z","iopub.execute_input":"2025-04-06T16:21:33.969596Z","iopub.status.idle":"2025-04-06T16:22:39.255128Z","shell.execute_reply.started":"2025-04-06T16:21:33.969567Z","shell.execute_reply":"2025-04-06T16:22:39.254229Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:22:58.189313Z","iopub.execute_input":"2025-04-06T16:22:58.189843Z","iopub.status.idle":"2025-04-06T16:23:03.184358Z","shell.execute_reply.started":"2025-04-06T16:22:58.189813Z","shell.execute_reply":"2025-04-06T16:23:03.183514Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\nCollecting openai\n  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.11.0a2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.29.0)\nDownloading openai-1.70.0-py3-none-any.whl (599 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: openai\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\nSuccessfully installed openai-1.70.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom tqdm.notebook import tqdm\nimport openai\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:23:08.963987Z","iopub.execute_input":"2025-04-06T16:23:08.964277Z","iopub.status.idle":"2025-04-06T16:23:46.440991Z","shell.execute_reply.started":"2025-04-06T16:23:08.964256Z","shell.execute_reply":"2025-04-06T16:23:46.440323Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def safe_process_chroma_results(results, process_func, default_message=\"No data available\"):\n    \"\"\"Safely process results from ChromaDB query\n    \n    Args:\n        results: The results from ChromaDB query\n        process_func: Function to process each metadata item\n        default_message: Default message if processing fails\n        \n    Returns:\n        Processed string representation\n    \"\"\"\n    try:\n        if not results or 'metadatas' not in results:\n            return default_message\n            \n        metadatas = results['metadatas']\n        if not metadatas:\n            return default_message\n            \n        \n        if isinstance(metadatas, list) and metadatas and isinstance(metadatas[0], dict):\n           \n            return \"\\n\".join([process_func(item) for item in metadatas])\n        elif isinstance(metadatas, list):\n            \n            if metadatas:\n                return f\"Data available but in unexpected format ({len(metadatas)} items)\"\n            else:\n                return default_message\n        else:\n            return default_message\n    except Exception as e:\n        print(f\"Error in safe_process_chroma_results: {e}\")\n        return default_message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:23:57.735632Z","iopub.execute_input":"2025-04-06T16:23:57.735947Z","iopub.status.idle":"2025-04-06T16:23:57.741485Z","shell.execute_reply.started":"2025-04-06T16:23:57.735924Z","shell.execute_reply":"2025-04-06T16:23:57.740644Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nclass StoryMemory:\n    def __init__(self, embedding_model_name=\"all-MiniLM-L6-v2\"):\n        \n        self.embedding_model = SentenceTransformer(embedding_model_name)\n        \n        \n        self.client = chromadb.Client()\n        \n        \n        try:\n            self.char_collection = self.client.get_or_create_collection(\"characters\")\n            self.plot_collection = self.client.get_or_create_collection(\"plots\")\n            self.setting_collection = self.client.get_or_create_collection(\"settings\")\n            \n            \n            self.subplot_collection = self.client.get_or_create_collection(\"subplots\")\n            self.theme_collection = self.client.get_or_create_collection(\"themes\")\n            \n            self.character_arc_collection = self.client.get_or_create_collection(\"character_arcs\")\n            \n        except Exception as e:\n            print(f\"Error initializing collections: {e}\")\n            \n            try:\n                self.client.delete_collection(\"characters\")\n                self.client.delete_collection(\"plots\")\n                self.client.delete_collection(\"settings\")\n                self.client.delete_collection(\"subplots\")\n                self.client.delete_collection(\"themes\")\n                self.client.delete_collection(\"character_arcs\")\n                \n                self.char_collection = self.client.create_collection(\"characters\")\n                self.plot_collection = self.client.create_collection(\"plots\")\n                self.setting_collection = self.client.create_collection(\"settings\")\n                self.subplot_collection = self.client.create_collection(\"subplots\")\n                self.theme_collection = self.client.create_collection(\"themes\")\n                self.character_arc_collection = self.client.create_collection(\"character_arcs\")\n            except Exception as e2:\n                print(f\"Error recreating collections: {e2}\")\n                raise\n        \n    def add_character(self, char_id, char_data):\n        \"\"\"Add or update character information in the memory\"\"\"\n        char_text = f\"Character {char_data['name']}: {json.dumps(char_data)}\"\n        \n        sanitized_metadata = {}\n        for key, value in char_data.items():\n            if isinstance(value, (str, int, float, bool)):\n                sanitized_metadata[key] = value\n            elif isinstance(value, list):\n                sanitized_metadata[key] = \", \".join(map(str, value))\n            else:\n                sanitized_metadata[key] = str(value)\n        \n        self.char_collection.add(\n            documents=[char_text],\n            metadatas=[sanitized_metadata],\n            ids=[f\"char_{char_id}\"]\n        )\n    \n    \n    def update_character_arc(self, char_name, episode_num, development):\n        \"\"\"Track character development/arc across episodes\"\"\"\n        arc_id = f\"arc_{char_name}_{episode_num}\"\n        arc_text = f\"Episode {episode_num} - {char_name}: {development}\"\n        \n        self.character_arc_collection.add(\n            documents=[arc_text],\n            metadatas=[{\n                \"character\": char_name,\n                \"episode\": episode_num,\n                \"development\": development\n            }],\n            ids=[arc_id]\n        )\n        \n    def add_plot_point(self, plot_id, episode, description, related_chars=None):\n        \"\"\"Add a plot point to memory\"\"\"\n        plot_text = f\"Episode {episode} - Plot point: {description}\"\n        \n        \n        if related_chars and isinstance(related_chars, list):\n            related_chars_str = \", \".join(related_chars)\n        else:\n            related_chars_str = str(related_chars) if related_chars else \"\"\n        \n        metadata = {\n            \"episode\": episode,\n            \"description\": description,\n            \"related_characters\": related_chars_str\n        }\n        \n        self.plot_collection.add(\n            documents=[plot_text],\n            metadatas=[metadata],\n            ids=[f\"plot_{plot_id}\"]\n        )\n\n            \n    def add_subplot(self, subplot_id, title, description, start_episode, characters_involved):\n        \"\"\"Add a subplot to memory\"\"\"\n        subplot_text = f\"Subplot: {title} - {description}\"\n        \n        metadata = {\n            \"title\": title,\n            \"description\": description,\n            \"start_episode\": start_episode,\n            \"status\": \"ongoing\",\n            \"characters_involved\": \", \".join(characters_involved) if isinstance(characters_involved, list) else characters_involved\n        }\n        \n        self.subplot_collection.add(\n            documents=[subplot_text],\n            metadatas=[metadata],\n            ids=[f\"subplot_{subplot_id}\"]\n        )\n    \n    \n    def update_subplot(self, subplot_id, status, resolution=None):\n        \"\"\"Update the status of a subplot (ongoing, resolved, etc.)\"\"\"\n\n        pass\n        \n    def add_setting(self, setting_id, setting_data):\n        \"\"\"Add setting information to memory\"\"\"\n        setting_text = f\"Setting: {json.dumps(setting_data)}\"\n        \n        \n        sanitized_metadata = {}\n        for key, value in setting_data.items():\n            if isinstance(value, (str, int, float, bool)):\n                sanitized_metadata[key] = value\n            elif isinstance(value, list):\n                sanitized_metadata[key] = \", \".join(map(str, value))\n            else:\n                sanitized_metadata[key] = str(value)\n        \n        self.setting_collection.add(\n            documents=[setting_text],\n            metadatas=[sanitized_metadata],\n            ids=[f\"setting_{setting_id}\"]\n        )\n    \n    def query_characters(self, query, n_results=5):\n        \"\"\"Query character information\"\"\"\n        results = self.char_collection.query(\n            query_texts=[query],\n            n_results=n_results\n        )\n        return results\n    \n    def query_plot_points(self, query, n_results=5):\n        \"\"\"Query plot information\"\"\"\n        results = self.plot_collection.query(\n            query_texts=[query],\n            n_results=n_results\n        )\n        return results\n    \n    \n    def get_episode_context(self, episode_num, max_plot_points=10):\n        \"\"\"Get context for generating a specific episode\"\"\"\n        \n        try:\n            all_chars = self.char_collection.get()\n        except Exception as e:\n            print(f\"Error getting characters: {e}\")\n            all_chars = {\"documents\": [], \"metadatas\": [], \"ids\": []}\n        \n        \n        query = f\"Important plot points for episode {episode_num}\"\n        \n        \n        if episode_num > 1:\n            try:\n                prev_plots = self.plot_collection.query(\n                    query_texts=[query],\n                    n_results=max_plot_points,\n                    where={\"episode\": {\"$lt\": episode_num}}\n                )\n            except Exception as e:\n                print(f\"Error querying plot points: {e}\")\n                prev_plots = {\"documents\": [], \"metadatas\": [], \"ids\": []}\n        else:\n            prev_plots = {\"documents\": [], \"metadatas\": [], \"ids\": []}\n            \n        \n        try:\n            active_subplots = self.subplot_collection.query(\n                query_texts=[f\"Relevant subplots for episode {episode_num}\"],\n                n_results=5\n            )\n        except Exception as e:\n            print(f\"Error getting subplots: {e}\")\n            active_subplots = {\"documents\": [], \"metadatas\": [], \"ids\": []}\n            \n        \n        try:\n            character_arcs = self.character_arc_collection.query(\n                query_texts=[f\"Character development before episode {episode_num}\"],\n                n_results=10,\n                where={\"episode\": {\"$lt\": episode_num}}\n            )\n        except Exception as e:\n            print(f\"Error getting character arcs: {e}\")\n            character_arcs = {\"documents\": [], \"metadatas\": [], \"ids\": []}\n        \n        \n        print(f\"Characters data structure: {list(all_chars.keys())}\")\n        print(f\"Plot points data structure: {list(prev_plots.keys())}\")\n        \n        \n        context = {\n            \"characters\": all_chars,\n            \"previous_plots\": prev_plots,\n            \"current_episode\": episode_num,\n            \"active_subplots\": active_subplots,\n            \"character_arcs\": character_arcs\n        }\n        \n        return context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:00.381674Z","iopub.execute_input":"2025-04-06T16:24:00.381971Z","iopub.status.idle":"2025-04-06T16:24:00.400648Z","shell.execute_reply.started":"2025-04-06T16:24:00.381949Z","shell.execute_reply":"2025-04-06T16:24:00.399742Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class StoryInputProcessor:\n    def __init__(self):\n        pass\n    \n    def process_initial_prompt(self, prompt):\n        \"\"\"Process the initial story prompt/concept\"\"\"\n        \n        story_elements = {\n            \"genre\": self._extract_genre(prompt),\n            \"setting\": self._extract_setting(prompt),\n            \"main_characters\": self._extract_characters(prompt),\n            \"theme\": self._extract_theme(prompt),\n            \"original_prompt\": prompt\n        }\n        return story_elements\n    \n    def _extract_genre(self, prompt):\n\n        genres = [\"fantasy\", \"sci-fi\", \"romance\", \"thriller\", \"comedy\", \"drama\", \"horror\", \"mystery\", \"adventure\"]\n        for genre in genres:\n            if genre.lower() in prompt.lower():\n                return genre\n        return \"general fiction\"  # Default\n    \n    def _extract_setting(self, prompt):\n        \n        if \"future\" in prompt.lower() or \"space\" in prompt.lower():\n            return \"futuristic\"\n        elif \"medieval\" in prompt.lower() or \"ancient\" in prompt.lower():\n            return \"historical\"\n        else:\n            return \"contemporary\"\n    \n    def _extract_characters(self, prompt):\n        \n        return []\n    \n    def _extract_theme(self, prompt):\n        themes = [\"love\", \"betrayal\", \"redemption\", \"survival\", \"growth\", \"friendship\"]\n        for theme in themes:\n            if theme.lower() in prompt.lower():\n                return theme\n        return \"journey\"  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:04.685420Z","iopub.execute_input":"2025-04-06T16:24:04.685753Z","iopub.status.idle":"2025-04-06T16:24:04.692774Z","shell.execute_reply.started":"2025-04-06T16:24:04.685728Z","shell.execute_reply":"2025-04-06T16:24:04.691726Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class OpenAIStoryGenerator:\n    def __init__(self, api_key, model=\"gpt-3.5-turbo\", max_tokens=1024):\n        openai.api_key = api_key\n        self.model = model\n        self.max_tokens = max_tokens\n\n    def generate_story_outline(self, story_elements):\n        \"\"\"Generate an overall story outline\"\"\"\n        prompt = f\"\"\"\n        Create a compelling story outline with the following elements:\n        Genre: {story_elements['genre']}\n        Setting: {story_elements['setting']}\n        Theme: {story_elements['theme']}\n        Number of episodes: 5\n\n        The outline should include:\n        1. Main characters with brief descriptions\n        2. Overall story arc\n        3. Brief summary of each episode\n        4. Key plot points and how they develop across episodes\n        5. At least 2 subplots that span multiple episodes\n        6. Character development arcs for main characters\n\n        Story Outline:\n        \"\"\"\n\n        response = openai.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            max_tokens=self.max_tokens,\n            temperature=0.7\n        )\n        return response.choices[0].message.content\n\n    def generate_episode(self, episode_num, story_outline, memory_context):\n        \"\"\"Generate a specific episode\"\"\"\n        \n\n        \n        characters = \"\"\n        if 'characters' in memory_context and 'metadatas' in memory_context['characters']:\n            try:\n                characters = \"\\n\".join([f\"- {char.get('name', 'Unknown')}: {char.get('description', 'No description')}\"\n                                       for char in memory_context['characters']['metadatas']])\n            except Exception as e:\n                print(f\"Error processing characters: {e}\")\n                characters = \"Characters available but couldn't be processed.\"\n                \n            if not characters:\n                characters = \"No character information available.\"\n\n       \n        previous_plots = \"\"\n        if episode_num > 1 and 'previous_plots' in memory_context:\n            \n            previous_plots = safe_process_chroma_results(\n                memory_context['previous_plots'],\n                lambda plot: f\"- Episode {plot.get('episode', '?')}: {plot.get('description', 'No description')}\",\n                \"No previous plot information available.\"\n            )\n\n        \n        active_subplots = \"\"\n        if 'active_subplots' in memory_context:\n            active_subplots = safe_process_chroma_results(\n                memory_context['active_subplots'],\n                lambda subplot: f\"- Subplot: {subplot.get('title', '?')}: {subplot.get('description', 'No description')} (Status: {subplot.get('status', 'ongoing')})\",\n                \"No active subplots.\"\n            )\n\n        \n        character_arcs = \"\"\n        if 'character_arcs' in memory_context:\n            character_arcs = safe_process_chroma_results(\n                memory_context['character_arcs'],\n                lambda arc: f\"- {arc.get('character', '?')} (Episode {arc.get('episode', '?')}): {arc.get('development', 'No development info')}\",\n                \"No character development information.\"\n            )\n\n        prompt = f\"\"\"\n        You are writing episode {episode_num} of a multi-episode story.\n\n        Story Outline: {story_outline}\n\n        Characters:\n        {characters}\n\n        Previous Important Events:\n        {previous_plots}\n        \n        Active Subplots:\n        {active_subplots}\n        \n        Character Development So Far:\n        {character_arcs}\n\n        Write Episode {episode_num} in script format with character dialogue and actions. \n        Make sure to maintain consistency with previous episodes while advancing the plot.\n        Ensure character personalities remain consistent with their established traits.\n        Continue developing subplots and character arcs naturally.\n        End the episode with a hook that leads into the next episode.\n\n        EPISODE {episode_num}:\n        \"\"\"\n\n        response = openai.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            max_tokens=self.max_tokens,\n            temperature=0.7\n        )\n        return response.choices[0].message.content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:06.697468Z","iopub.execute_input":"2025-04-06T16:24:06.697812Z","iopub.status.idle":"2025-04-06T16:24:06.707314Z","shell.execute_reply.started":"2025-04-06T16:24:06.697786Z","shell.execute_reply":"2025-04-06T16:24:06.706284Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class LongFormStoryGenerator(OpenAIStoryGenerator):\n    def __init__(self, api_key, model=\"gpt-3.5-turbo\", max_tokens=1024):\n        super().__init__(api_key, model, max_tokens)\n        \n    def generate_episode_with_chunking(self, episode_num, story_outline, memory_context, max_chunk_length=10000):\n        \"\"\"Generate a long-form episode by creating it in manageable chunks\"\"\"\n        \n        episode_outline = self._generate_episode_outline(episode_num, story_outline, memory_context)\n        \n        \n        scenes = self._break_into_scenes(episode_outline)\n        \n        \n        complete_episode = f\"EPISODE {episode_num}:\\n\\n\"\n        \n        for i, scene in enumerate(scenes):\n            print(f\"Generating scene {i+1} of {len(scenes)}...\")\n            scene_prompt = f\"\"\"\n            You are writing scene {i+1} of episode {episode_num}.\n            \n            Scene outline: {scene}\n            \n            Characters:\n            {self._extract_characters_for_scene(scene, memory_context)}\n            \n            Write this scene in detail with rich dialogue and descriptions. \n            This is part of a longer episode, so focus only on this scene.\n            \n            SCENE {i+1}:\n            \"\"\"\n            \n            scene_content = self._call_openai_api(scene_prompt)\n            complete_episode += f\"\\n\\n[SCENE {i+1}]\\n{scene_content}\\n\\n\"\n            \n        return complete_episode\n    \n    def _generate_episode_outline(self, episode_num, story_outline, memory_context):\n        \"\"\"Generate just the outline/plan for the episode\"\"\"\n        \n        previous_episodes_summary = safe_process_chroma_results(\n            memory_context.get('previous_plots', {}),\n            lambda plot: f\"- Episode {plot.get('episode', '?')}: {plot.get('description', 'No description')}\",\n            \"No previous episode information.\"\n        )\n        \n        prompt = f\"\"\"\n        Create a detailed scene-by-scene outline for episode {episode_num}.\n        \n        Story Outline: {story_outline}\n        \n        Previous Episodes Summary:\n        {previous_episodes_summary}\n        \n        For each scene, include:\n        1. Setting\n        2. Characters present\n        3. Brief description of what happens\n        4. How it advances the plot or character development\n        \n        Format each scene like this:\n        SCENE X: [brief title]\n        Setting: [location]\n        Characters: [list of characters]\n        Action: [what happens]\n        Purpose: [how it advances story]\n        \n        Create 5-8 scenes for this episode.\n        \"\"\"\n        \n        response = self._call_openai_api(prompt)\n        return response\n        \n    def _break_into_scenes(self, episode_outline):\n        \"\"\"Analyze the outline and break it into logical scene chunks\"\"\"\n        \n        import re\n        scene_pattern = r'SCENE \\d+:'\n        scenes = re.split(scene_pattern, episode_outline)\n        \n        \n        if scenes and scenes[0].strip() == '':\n            scenes = scenes[1:]\n            \n        \n        for i in range(len(scenes)):\n            scenes[i] = f\"SCENE {i+1}:{scenes[i]}\"\n            \n        return scenes\n    \n    def _extract_characters_for_scene(self, scene, memory_context):\n        \"\"\"Extract which characters are likely in this scene\"\"\"\n        character_info = \"\"\n        \n        if 'characters' in memory_context and 'metadatas' in memory_context['characters']:\n            try:\n                \n                chars_in_scene = []\n                for char in memory_context['characters']['metadatas']:\n                    char_name = char.get('name', '')\n                    if char_name and char_name in scene:\n                        chars_in_scene.append(f\"- {char_name}: {char.get('description', 'No description')}\")\n                \n                character_info = \"\\n\".join(chars_in_scene)\n            except Exception as e:\n                print(f\"Error extracting characters for scene: {e}\")\n                character_info = \"Character information available but couldn't be processed.\"\n        \n        return character_info or \"No specific character information available.\"\n    \n    def _call_openai_api(self, prompt):\n        \"\"\"Helper method to call OpenAI API\"\"\"\n        response = openai.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            max_tokens=self.max_tokens,\n            temperature=0.7\n        )\n        return response.choices[0].message.content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:09.778180Z","iopub.execute_input":"2025-04-06T16:24:09.778501Z","iopub.status.idle":"2025-04-06T16:24:09.787893Z","shell.execute_reply.started":"2025-04-06T16:24:09.778464Z","shell.execute_reply":"2025-04-06T16:24:09.787004Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nclass StoryPostProcessor:\n    def __init__(self, memory):\n        self.memory = memory\n        \n        try:\n            self.ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n            self.use_ner = True\n            print(\"NER model loaded successfully!\")\n        except Exception as e:\n            print(f\"Error loading NER model: {e}\")\n            self.use_ner = False\n        \n    def extract_and_store_elements(self, episode_num, episode_content):\n        \"\"\"Extract story elements from generated content and store in memory\"\"\"\n        \n        characters = self._extract_characters(episode_content)\n        for i, char in enumerate(characters):\n            \n            sanitized_char = self._sanitize_metadata(char)\n            self.memory.add_character(f\"{episode_num}_{i}\", sanitized_char)\n        \n        \n        plot_points = self._extract_plot_points(episode_content)\n        for i, plot in enumerate(plot_points):\n            \n            if isinstance(plot, str):\n                related_char_names = [char[\"name\"] for char in characters]\n                self.memory.add_plot_point(\n                    f\"{episode_num}_{i}\",\n                    episode_num,\n                    plot,\n                    related_char_names\n                )\n        \n        \n        subplots = self._extract_subplots(episode_content, episode_num)\n        for i, subplot in enumerate(subplots):\n            self.memory.add_subplot(\n                f\"{episode_num}_{i}\",\n                subplot[\"title\"],\n                subplot[\"description\"],\n                episode_num,\n                subplot[\"characters\"]\n            )\n            \n        \n        character_developments = self._extract_character_developments(episode_content, characters)\n        for char_name, development in character_developments.items():\n            self.memory.update_character_arc(char_name, episode_num, development)\n        \n        return {\n            \"characters\": characters,\n            \"plot_points\": plot_points,\n            \"subplots\": subplots,\n            \"character_developments\": character_developments\n        }\n    \n    def _sanitize_metadata(self, metadata_dict):\n        \"\"\"Convert all values to simple types that ChromaDB accepts\"\"\"\n        sanitized = {}\n        for key, value in metadata_dict.items():\n            if isinstance(value, (str, int, float, bool)):\n                sanitized[key] = value\n            elif isinstance(value, list):\n                sanitized[key] = \", \".join(map(str, value))\n            elif isinstance(value, dict):\n                sanitized[key] = json.dumps(value)\n            else:\n                sanitized[key] = str(value)\n        return sanitized\n    \n    def _extract_characters(self, text):\n        \"\"\"Extract character information from text\"\"\"\n        if self.use_ner:\n            return self._extract_characters_with_ner(text)\n        else:\n            return self._extract_characters_with_regex(text)\n            \n    def _extract_characters_with_ner(self, text):\n        \"\"\"Extract characters using NER pipeline\"\"\"\n        \n        try:\n            results = self.ner_pipeline(text[:10000])  \n            \n            \n            characters = []\n            current_entity = {\"name\": \"\", \"type\": \"\"}\n            \n            for entity in results:\n                if entity[\"entity\"].startswith(\"B-PER\"):\n                    \n                    if current_entity[\"name\"] and current_entity[\"type\"] == \"PER\":\n                        characters.append({\"name\": current_entity[\"name\"].strip()})\n                    current_entity = {\"name\": entity[\"word\"], \"type\": \"PER\"}\n                elif entity[\"entity\"].startswith(\"I-PER\") and current_entity[\"type\"] == \"PER\":\n                    \n                    current_entity[\"name\"] += \" \" + entity[\"word\"]\n            \n            \n            if current_entity[\"name\"] and current_entity[\"type\"] == \"PER\":\n                characters.append({\"name\": current_entity[\"name\"].strip()})\n                \n            \n            unique_characters = []\n            seen_names = set()\n            for char in characters:\n                if char[\"name\"] not in seen_names:\n                    seen_names.add(char[\"name\"])\n                    char[\"description\"] = f\"Character appearing in the story\"\n                    unique_characters.append(char)\n                    \n            return unique_characters\n        except Exception as e:\n            print(f\"Error in NER character extraction: {e}\")\n            \n            return self._extract_characters_with_regex(text)\n    \n    def _extract_characters_with_regex(self, text):\n        \"\"\"Extract character information using regex (fallback method)\"\"\"\n        \n        import re\n        dialogue_pattern = r'([A-Z][A-Za-z\\s]+):'\n        character_names = list(set(re.findall(dialogue_pattern, text)))\n        \n        characters = []\n        for name in character_names:\n            name = name.strip()\n            if name and len(name) > 1:  \n                characters.append({\n                    \"name\": name,\n                    \"description\": f\"Character appearing in the story\",\n                    \"appearances\": \"yes\"  \n                })\n        \n        return characters\n    \n    def _extract_plot_points(self, text):\n        \"\"\"Extract key plot points from an episode\"\"\"\n        \n        \n        \n        \n        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n        \n        \n        \n        plot_points = []\n        for i, line in enumerate(lines):\n            \n            if ':' not in line and len(line) > 50:\n                plot_points.append(line[:100] + \"...\")  \n                \n            if len(plot_points) >= 3:  \n                break\n                \n        return plot_points\n    \n    \n    def _extract_subplots(self, text, episode_num):\n        \"\"\"Extract subplot information from the episode\"\"\"\n        \n        \n        \n        \n        subplot_keywords = [\n            \"meanwhile\", \"on the other hand\", \"elsewhere\", \n            \"at the same time\", \"subplot\", \"side story\"\n        ]\n        \n        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n        \n        subplots = []\n        for keyword in subplot_keywords:\n            for i, line in enumerate(lines):\n                if keyword.lower() in line.lower() and len(line) > 30:\n                    \n                    if i < len(lines) - 1:\n                        context = lines[i] + \" \" + lines[i+1]\n                    else:\n                        context = lines[i]\n                    \n                    \n                    chars = self._extract_characters_with_regex(context)\n                    char_names = [c[\"name\"] for c in chars]\n                    \n                    subplots.append({\n                        \"title\": f\"Subplot from Episode {episode_num}\",\n                        \"description\": context[:100] + \"...\" if len(context) > 100 else context,\n                        \"characters\": char_names\n                    })\n        \n        \n        unique_subplots = []\n        seen_descriptions = set()\n        for subplot in subplots:\n            if subplot[\"description\"] not in seen_descriptions:\n                seen_descriptions.add(subplot[\"description\"])\n                unique_subplots.append(subplot)\n                \n            if len(unique_subplots) >= 2:  \n                break\n                \n        return unique_subplots\n    \n    \n    def _extract_character_developments(self, text, characters):\n        \"\"\"Extract character development information\"\"\"\n        developments = {}\n        \n        for character in characters:\n            char_name = character[\"name\"]\n            \n            char_mentions = []\n            \n            \n            paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n            \n            for paragraph in paragraphs:\n                if char_name in paragraph and len(paragraph) > 50 and \":\" not in paragraph[:len(char_name)+1]:\n                    char_mentions.append(paragraph)\n            \n            \n            if char_mentions:\n                longest_mention = max(char_mentions, key=len)\n                developments[char_name] = longest_mention[:150] + \"...\" if len(longest_mention) > 150 else longest_mention\n        \n        return developments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:12.745836Z","iopub.execute_input":"2025-04-06T16:24:12.746168Z","iopub.status.idle":"2025-04-06T16:24:12.764695Z","shell.execute_reply.started":"2025-04-06T16:24:12.746140Z","shell.execute_reply":"2025-04-06T16:24:12.763884Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nclass AIStorytellingPipeline:\n    def __init__(self, use_openai=False, openai_api_key=None, use_long_form=False):\n        \n        self.memory = StoryMemory()\n        self.input_processor = StoryInputProcessor()\n        \n        \n        if use_openai:\n            if use_long_form:\n                self.story_generator = LongFormStoryGenerator(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n            else:\n                self.story_generator = OpenAIStoryGenerator(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n        else:\n            \n            self.story_generator = OpenAIStoryGenerator(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n            \n        self.post_processor = StoryPostProcessor(self.memory)\n        self.story_data = {}\n        self.use_long_form = use_long_form\n        \n    def create_story(self, initial_prompt, num_episodes=5):\n        \"\"\"Create a complete story based on the initial prompt\"\"\"\n        \n        story_elements = self.input_processor.process_initial_prompt(initial_prompt)\n        \n        \n        print(\"Generating story outline...\")\n        story_outline = self.story_generator.generate_story_outline(story_elements)\n        self.story_data[\"outline\"] = story_outline\n        \n        \n        initial_elements = self.post_processor.extract_and_store_elements(0, story_outline)\n        \n        \n        self.story_data[\"episodes\"] = []\n        for ep_num in range(1, num_episodes + 1):\n            print(f\"Generating episode {ep_num}...\")\n            \n           \n            memory_context = self.memory.get_episode_context(ep_num)\n            \n            \n            if self.use_long_form and hasattr(self.story_generator, 'generate_episode_with_chunking'):\n                episode_content = self.story_generator.generate_episode_with_chunking(ep_num, story_outline, memory_context)\n            else:\n                episode_content = self.story_generator.generate_episode(ep_num, story_outline, memory_context)\n            \n            \n            extracted_elements = self.post_processor.extract_and_store_elements(ep_num, episode_content)\n            \n            \n            self.story_data[\"episodes\"].append({\n                \"episode_number\": ep_num,\n                \"content\": episode_content,\n                \"extracted_elements\": extracted_elements\n            })\n            \n            print(f\"Episode {ep_num} completed.\")\n        \n        return self.story_data\n    \n    def save_story(self, filename=\"generated_story.json\"):\n        \"\"\"Save the generated story to a file\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.story_data, f, indent=2)\n        print(f\"Story saved to {filename}\")\n    \n    def load_story(self, filename=\"generated_story.json\"):\n        \"\"\"Load a previously generated story\"\"\"\n        with open(filename, 'r') as f:\n            self.story_data = json.load(f)\n        print(f\"Story loaded from {filename}\")\n        \n    def get_story_analytics(self):\n        \"\"\"Generate analytics about the story\"\"\"\n        if not self.story_data or \"episodes\" not in self.story_data:\n            return {\"error\": \"No story data available\"}\n            \n        analytics = {\n            \"total_episodes\": len(self.story_data[\"episodes\"]),\n            \"character_consistency\": self._analyze_character_consistency(),\n            \"subplot_progression\": self._analyze_subplot_progression(),\n            \"word_count\": self._calculate_word_count(),\n            \"character_count\": self._count_characters()\n        }\n        \n        return analytics\n    \n    def _analyze_character_consistency(self):\n        \"\"\"Analyze how consistent characters are across episodes\"\"\"\n        if not self.story_data or \"episodes\" not in self.story_data:\n            return {}\n            \n        \n        character_appearances = {}\n        \n        for ep in self.story_data[\"episodes\"]:\n            ep_num = ep[\"episode_number\"]\n            if \"extracted_elements\" in ep and \"characters\" in ep[\"extracted_elements\"]:\n                for char in ep[\"extracted_elements\"][\"characters\"]:\n                    char_name = char[\"name\"]\n                    if char_name not in character_appearances:\n                        character_appearances[char_name] = []\n                    character_appearances[char_name].append(ep_num)\n        \n        \n        results = {\n            \"total_characters\": len(character_appearances),\n            \"recurring_characters\": sum(1 for chars in character_appearances.values() if len(chars) > 1),\n            \"character_details\": character_appearances\n        }\n        \n        return results\n    \n    def _analyze_subplot_progression(self):\n        \"\"\"Analyze how subplots progress across episodes\"\"\"\n        \n        return {\"subplot_analysis\": \"Placeholder for subplot analysis\"}\n    \n    def _calculate_word_count(self):\n        \"\"\"Calculate total word count for the story\"\"\"\n        if not self.story_data:\n            return 0\n            \n        word_count = 0\n        \n        \n        if \"outline\" in self.story_data:\n            word_count += len(self.story_data[\"outline\"].split())\n            \n        \n        if \"episodes\" in self.story_data:\n            for ep in self.story_data[\"episodes\"]:\n                if \"content\" in ep:\n                    word_count += len(ep[\"content\"].split())\n                    \n        return word_count\n    \n    def _count_characters(self):\n        \"\"\"Count unique characters in the story\"\"\"\n        if not self.story_data or \"episodes\" not in self.story_data:\n            return 0\n            \n        unique_chars = set()\n        \n        for ep in self.story_data[\"episodes\"]:\n            if \"extracted_elements\" in ep and \"characters\" in ep[\"extracted_elements\"]:\n                for char in ep[\"extracted_elements\"][\"characters\"]:\n                    unique_chars.add(char[\"name\"])\n                    \n        return len(unique_chars)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:17.355107Z","iopub.execute_input":"2025-04-06T16:24:17.355498Z","iopub.status.idle":"2025-04-06T16:24:17.369920Z","shell.execute_reply.started":"2025-04-06T16:24:17.355466Z","shell.execute_reply":"2025-04-06T16:24:17.369029Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nclass StoryControlPanel:\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n        \n    def create_ui(self):\n        with gr.Blocks() as app:\n            gr.Markdown(\"# AI Story Generator\")\n            \n            with gr.Tab(\"Generate Story\"):\n                prompt_input = gr.Textbox(label=\"Story Prompt\", lines=5)\n                num_episodes = gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of Episodes\")\n                \n                with gr.Row():\n                    generate_btn = gr.Button(\"Generate Story\")\n                    \n                with gr.Accordion(\"Advanced Settings\", open=False):\n                    model_choice = gr.Dropdown([\"gpt-3.5-turbo\", \"gpt-4\"], label=\"OpenAI Model\")\n                    use_long_form = gr.Checkbox(label=\"Use Long-Form Generation\", value=False)\n                    \n                story_output = gr.Textbox(label=\"Generated Story\", lines=20)\n                \n            with gr.Tab(\"Story Memory\"):\n                with gr.Row():\n                    character_list = gr.Dataframe(label=\"Characters\")\n                    plot_points = gr.Dataframe(label=\"Plot Points\")\n                \n            with gr.Tab(\"Analytics\"):\n                analytics_output = gr.JSON(label=\"Story Analytics\")\n                analyze_btn = gr.Button(\"Analyze Story\")\n                \n            generate_btn.click(\n                fn=self.generate_story,\n                inputs=[prompt_input, num_episodes, model_choice, use_long_form],\n                outputs=[story_output, character_list, plot_points]\n            )\n            \n            analyze_btn.click(\n                fn=self.analyze_story,\n                inputs=[],\n                outputs=[analytics_output]\n            )\n            \n        return app\n    \n    def generate_story(self, prompt, num_episodes, model, use_long_form):\n        \n        self.pipeline.story_generator.model = model\n        self.pipeline.use_long_form = use_long_form\n        \n        \n        story_data = self.pipeline.create_story(prompt, num_episodes=num_episodes)\n        \n       \n        story_text = f\"## STORY OUTLINE\\n\\n{story_data['outline']}\\n\\n\"\n        for i, episode in enumerate(story_data[\"episodes\"]):\n            story_text += f\"\\n\\n## EPISODE {i+1}\\n\\n{episode['content']}\\n\\n\"\n            \n        \n        characters = []\n        for ep in story_data[\"episodes\"]:\n            if \"extracted_elements\" in ep and \"characters\" in ep[\"extracted_elements\"]:\n                for char in ep[\"extracted_elements\"][\"characters\"]:\n                    characters.append({\n                        \"Name\": char[\"name\"],\n                        \"Episode\": ep[\"episode_number\"],\n                        \"Description\": char.get(\"description\", \"\")\n                    })\n        \n        plots = []\n        for ep in story_data[\"episodes\"]:\n            if \"extracted_elements\" in ep and \"plot_points\" in ep[\"extracted_elements\"]:\n                for plot in ep[\"extracted_elements\"][\"plot_points\"]:\n                    plots.append({\n                        \"Episode\": ep[\"episode_number\"],\n                        \"Description\": plot\n                    })\n        \n        return story_text, characters, plots\n    \n    def analyze_story(self):\n        analytics = self.pipeline.get_story_analytics()\n        return analytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:20.587084Z","iopub.execute_input":"2025-04-06T16:24:20.587385Z","iopub.status.idle":"2025-04-06T16:24:20.597753Z","shell.execute_reply.started":"2025-04-06T16:24:20.587362Z","shell.execute_reply":"2025-04-06T16:24:20.596878Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\ndef run_pipeline(api_key, initial_prompt, num_episodes=3, use_long_form=False, show_ui=False):\n    \n    pipeline = AIStorytellingPipeline(use_openai=True, openai_api_key=api_key, use_long_form=use_long_form)\n    \n    if show_ui:\n        \n        control_panel = StoryControlPanel(pipeline)\n        ui = control_panel.create_ui()\n        ui.launch(share=True)\n    else:\n        \n        try:\n            story_data = pipeline.create_story(initial_prompt, num_episodes=num_episodes)\n            \n            \n            pipeline.save_story()\n            \n            \n            for i, episode in enumerate(story_data[\"episodes\"]):\n                print(f\"\\n\\n=== EPISODE {i+1} ===\")\n                print(episode[\"content\"])\n                \n            \n            analytics = pipeline.get_story_analytics()\n            print(\"\\n\\n=== STORY ANALYTICS ===\")\n            print(json.dumps(analytics, indent=2))\n            \n            return story_data\n            \n        except Exception as e:\n            print(f\"Error during story generation: {e}\")\n            \n            \n            try:\n                if hasattr(pipeline, 'story_data') and pipeline.story_data:\n                    pipeline.save_story(\"partial_story.json\")\n                    print(f\"Partial story saved to partial_story.json\")\n                    \n                    \n                    for i, episode in enumerate(pipeline.story_data.get(\"episodes\", [])):\n                        print(f\"\\n\\n=== EPISODE {i+1} ===\")\n                        print(episode[\"content\"])\n            except Exception as save_error:\n                print(f\"Error saving partial story: {save_error}\")\n            \n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:24:24.421299Z","iopub.execute_input":"2025-04-06T16:24:24.421670Z","iopub.status.idle":"2025-04-06T16:24:24.428776Z","shell.execute_reply.started":"2025-04-06T16:24:24.421642Z","shell.execute_reply":"2025-04-06T16:24:24.427838Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"openai_api_key = \"YOUR_API_KEY\"  # Your OpenAI API key\n\ninitial_prompt = \"\"\"\nCreate a sci-fi adventure about a team of explorers who discover a hidden civilization \non a distant planet. The story should have elements of mystery, friendship, and betrayal.\n\"\"\"\n\n# For regular generation (comment this out):\n# run_pipeline(openai_api_key, initial_prompt, num_episodes=5)\n\n# For long-form generation with UI (uncomment this):\nrun_pipeline(openai_api_key, initial_prompt, num_episodes=5, use_long_form=True, show_ui=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:38:02.268188Z","iopub.execute_input":"2025-04-06T16:38:02.268578Z","iopub.status.idle":"2025-04-06T16:38:12.302205Z","shell.execute_reply.started":"2025-04-06T16:38:02.268549Z","shell.execute_reply":"2025-04-06T16:38:12.301353Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e03833e193b4e5380fd39d9d2ecafce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5283e0b9e96d4a57ac2b059b9cbf42cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad9d68b69714f749c4531d2a73330fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecaf7af4e106419e9f4c9aa2c1180ebf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f591496efd94431ea8cc24e0ae28e3fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe2b8c907c144b98f5a441e3f7a0f36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff5c7a73ded4b418e8c997888cab206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69762d5846e24b37b559abf5b60f1ca1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94bf21ceca94a2dae62aa6391a9e206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92971004ccd64ca6b1c789cad7082e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f70185a7b9145ca9c064386bd431e66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb99fab682e2439fac036b28f323d490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d39313ff3d2405588b79f2342d3c7f0"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d675e5de31ab48bd9e1906ca6684e143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28dc0a8a7c94f87b8ed35792bdbd3d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99eb765c4d6419380f56280acc61472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab91bb0c15c44f80bdeec4023864cbfe"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"NER model loaded successfully!\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://337ef97a2f8ef9ca15.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://337ef97a2f8ef9ca15.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Generating story outline...\n","output_type":"stream"},{"name":"stderr","text":"/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:01<00:00, 74.0MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Generating episode 1...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['documents', 'metadatas', 'ids']\nEpisode 1 completed.\nGenerating episode 2...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 2 completed.\nGenerating episode 3...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 3 completed.\nGenerating episode 4...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 4 completed.\nGenerating episode 5...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 5 completed.\nGenerating episode 6...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 6 completed.\nGenerating episode 7...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 7 completed.\nGenerating episode 8...\nCharacters data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas']\nPlot points data structure: ['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas', 'distances']\nEpisode 8 completed.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Github- https://github.com/dasdebanna/storytelling","metadata":{}},{"cell_type":"code","source":"# Read the saved story\nimport json\n\nwith open('generated_story.json', 'r') as f:\n    saved_story = json.load(f)\n\n# Print all episodes\nfor i, episode in enumerate(saved_story[\"episodes\"]):\n    print(f\"\\n\\n=== EPISODE {i+1} ===\")\n    print(episode[\"content\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:09:00.720043Z","iopub.execute_input":"2025-04-04T17:09:00.720340Z","iopub.status.idle":"2025-04-04T17:09:00.727214Z","shell.execute_reply.started":"2025-04-04T17:09:00.720319Z","shell.execute_reply":"2025-04-04T17:09:00.726475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}